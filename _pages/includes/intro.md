[//]: # (**Rongjie Huang &#40;ÈªÑËûçÊù∞&#41;** is the final year's graduate student at College of Computer Science and Software, [Zhejiang University]&#40;https://www.zju.edu.cn/english/&#41;, supervised by [Prof. Zhou Zhao]&#40;https://person.zju.edu.cn/zhaozhou&#41;. I also obtained Bachelor‚Äôs degree at Zhejiang University. During my graduate study, I was lucky to collaborate with the CMU Speech Team led by [Prof. Shinji Watanabe]&#40;https://scholar.google.com/citations?user=U5xRA6QAAAAJ&#41;, and Audio Research Team at Zhejiang University. I was grateful to intern or collaborate at TikTok, Shanghai AI Lab, Tencent Seattle Lab, Alibaba Qwen, with [Yi Ren]&#40;https://github.com/RayeRen&#41;, [Jinglin Liu]&#40;https://github.com/MoonInTheRiver&#41;, [Chunlei Zhang]&#40;https://scholar.google.com/citations?user=NCKZGb0AAAAJ&#41; and [Dong Yu]&#40;https://scholar.google.com/citations?user=tMY31_gAAAAJ&#41;.)
I am about to receive my PhD in June 2025 from the School of Computer Science and Technology at Zhejiang University, under the supervision of Professor [Fei Wu (Âê¥È£û)](https://person.zju.edu.cn/wufei). From December 2023 to January 2025, I was a visiting researcher at the NExT++ Research Center at the National University of Singapore, under the supervision of Professor [Tat-Seng Chua (Ëî°ËææÊàê)](https://www.chuatatseng.com/).

[//]: # (My research interest includes **Multi-Modal Generative AI, Multi-Modal Language Processing, and AI4Science**. I have published **first-author papers** at the top international AI conferences such as **NeurIPS/ICLR/ICML/ACL/IJCAI**. I developed a few well-known Speech/NLP algorithms including:)

[//]: # (- AudioGPT, UniAudio, Make-A-Voice: Multitask, Multilingual LLMs)

[//]: # (- Make-An-Audio, GenerSpeech: Zero-shot text-guided synthesis)

[//]: # (- FastDiff 1/2, ProDiff: AIGC diffusion models)

[//]: # (- TranSpeech, and AV-TranSpeech: Multimodal Translation)

My main research areas are:
- (Multimodal) Large Language Models, including self-reflection, quantization, applications, etc.
- Device-Cloud/Large-Small Model Collaborative Learning.
- Information Retrieval/Recommendation.

[//]: # (In 2024, I lead or participate in the following research topics:)

[//]: # (- Speech/NLP: multimodal generation and translation)

[//]: # (- Large Language Models &#40;LLMs&#41;: Audio/Visual)

[//]: # (- Diffusion models: Image/Audio/3D)


# üî• News

<style>
  .scrollable {
    max-height: 260px; /* ËÆæÁΩÆÊúÄÂ§ßÈ´òÂ∫¶ */
    overflow-y: scroll; /* ËÆæÁΩÆÂûÇÁõ¥ÊªöÂä®Êù° */
  }
</style>

[//]: # (2024.01~)
[//]: # (Recent 3 years,)
<div class="scrollable">
  <ul>
    <li><strong>2025.01</strong>: 1 paper is selected for AAAI 2025 oral presentation </li>
    <li><strong>2025.01</strong>: 1 paper is accepted by WWW 2025 (Research Track) </li>
    <li><strong>2024.12</strong>: 2 paper is accepted by AAAI 2025 (1 Main, 1 Student abstract) </li>
    <li><strong>2024.11</strong>: 1 paper is accepted by TOMM 2025 </li>
    <li><strong>2024.11</strong>: 2 papers are accepted by KDD 2025 (Research Track) </li>
    <li><strong>2024.07</strong>: 1 paper is selected for ACM MM 2024 oral presentation </li>
    <li><strong>2024.07</strong>: 2 papers are accepted by ACM MM 2024  </li>
    <li><strong>2024.05</strong>: 1 paper is accepted by KDD 2024 (Research Track) </li>
    <li><strong>2024.02</strong>: 1 paper is accepted by CVPR 2024  </li>
    <li><strong>2024.02</strong>: 1 paper is accepted by ICLR 2024  </li>
    <li><strong>2024.01</strong>: 1 paper is selected for WWW 2024 oral presentation </li>
    <li><strong>2024.01</strong>: 1 paper is accepted by WWW 2024 (Research Track)  </li>
    <li><strong>2023.12</strong>: 1 paper is accepted by AAAI 2023 (Main)  </li>
    <li><strong>2023.09</strong>: 1 paper is accepted by EMNLP 2023  </li>
    <li><strong>2023.07</strong>: 1 paper is accepted by CICAI 2023  </li>
    <li><strong>2023.04</strong>: 1 paper is accepted by FITEE 2023  </li>
    <li><strong>2023.01</strong>: 1 paper is accepted by WWW 2023 (Research Track)  </li>
  </ul>
</div>
