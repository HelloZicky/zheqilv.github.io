[//]: # (**Rongjie Huang &#40;ÈªÑËûçÊù∞&#41;** is the final year's graduate student at College of Computer Science and Software, [Zhejiang University]&#40;https://www.zju.edu.cn/english/&#41;, supervised by [Prof. Zhou Zhao]&#40;https://person.zju.edu.cn/zhaozhou&#41;. I also obtained Bachelor‚Äôs degree at Zhejiang University. During my graduate study, I was lucky to collaborate with the CMU Speech Team led by [Prof. Shinji Watanabe]&#40;https://scholar.google.com/citations?user=U5xRA6QAAAAJ&#41;, and Audio Research Team at Zhejiang University. I was grateful to intern or collaborate at TikTok, Shanghai AI Lab, Tencent Seattle Lab, Alibaba Qwen, with [Yi Ren]&#40;https://github.com/RayeRen&#41;, [Jinglin Liu]&#40;https://github.com/MoonInTheRiver&#41;, [Chunlei Zhang]&#40;https://scholar.google.com/citations?user=NCKZGb0AAAAJ&#41; and [Dong Yu]&#40;https://scholar.google.com/citations?user=tMY31_gAAAAJ&#41;.)
I am about to receive my PhD in June 2025 from the School of Computer Science and Technology at Zhejiang University, under the supervision of Professor Fei Wu. From December 2023 to January 2025, I was a visiting researcher at the NExT++ Research Center at the National University of Singapore, under the supervision of Professor Tat-Seng Chua.

[//]: # (My research interest includes **Multi-Modal Generative AI, Multi-Modal Language Processing, and AI4Science**. I have published **first-author papers** at the top international AI conferences such as **NeurIPS/ICLR/ICML/ACL/IJCAI**. I developed a few well-known Speech/NLP algorithms including:)

[//]: # (- AudioGPT, UniAudio, Make-A-Voice: Multitask, Multilingual LLMs)

[//]: # (- Make-An-Audio, GenerSpeech: Zero-shot text-guided synthesis)

[//]: # (- FastDiff 1/2, ProDiff: AIGC diffusion models)

[//]: # (- TranSpeech, and AV-TranSpeech: Multimodal Translation)

My main research areas are:
- (Multimodal) Large Language Models, including self-reflection, quantization, applications, etc.
- Device-Cloud/Large-Small Model Collaborative Learning.
- Information Retrieval/Recommendation.

[//]: # (In 2024, I lead or participate in the following research topics:)

[//]: # (- Speech/NLP: multimodal generation and translation)

[//]: # (- Large Language Models &#40;LLMs&#41;: Audio/Visual)

[//]: # (- Diffusion models: Image/Audio/3D)


# üî• News

<style>
  .scrollable {
    max-height: 260px; /* ËÆæÁΩÆÊúÄÂ§ßÈ´òÂ∫¶ */
    overflow-y: scroll; /* ËÆæÁΩÆÂûÇÁõ¥ÊªöÂä®Êù° */
  }
</style>

[//]: # (2024.01~)
[//]: # (Recent 3 years,)
<div class="scrollable">
  <ul>
    <li><strong>2025.01</strong>: 1 paper are selected to AAAI 2025 oral presentation </li>
    <li><strong>2025.01</strong>: 1 paper are accepted by WWW 2025 </li>
    <li><strong>2024.12</strong>: 1 paper are accepted by AAAI 2025 (Main) </li>
    <li><strong>2024.11</strong>: 1 paper is accepted by TOMM 2025 </li>
    <li><strong>2024.11</strong>: 2 papers are accepted by KDD 2025 (Research Track) </li>
    <li><strong>2024.07</strong>: 2 papers are accepted by ACM MM 2024  </li>
    <li><strong>2024.05</strong>: 1 paper are accepted by KDD 2024  </li>
    <li><strong>2024.02</strong>: 1 paper are accepted by CVPR 2024  </li>
    <li><strong>2024.02</strong>: 1 paper are accepted by ICLR 2024  </li>
    <li><strong>2024.01</strong>: 1 paper are selected to WWW 2024 oral presentation </li>
    <li><strong>2024.01</strong>: 1 paper are accepted by WWW 2024  </li>
    <li><strong>2023.12</strong>: 1 paper are accepted by AAAI 2023  </li>
    <li><strong>2023.09</strong>: 1 paper are accepted by EMNLP 2023  </li>
    <li><strong>2023.07</strong>: 1 paper are accepted by CICAI 2023  </li>
    <li><strong>2023.04</strong>: 1 paper are accepted by FITEE 2023  </li>
    <li><strong>2023.01</strong>: 1 paper are accepted by WWW 2023  </li>
  </ul>
</div>

[//]: # (<div class="scrollable">)

[//]: # (  <ul>)

[//]: # (    <li><strong>2024.05</strong>: 6 papers are accepted by ACL 2024! &#40;main conference and findings&#41;! Thanks to my co-authors! </li>)

[//]: # (    <li><strong>2024.05</strong>: 3 papers are accepted by ICML 2024!</li>)

[//]: # (    <li><strong>2024.03</strong>: 1 paper is accepted by NAACL 2024 main conference!</li>)

[//]: # (    <li><strong>2024.01</strong>: 1 paper is accepted by ICLR 2024!</li>)

[//]: # (    <li><strong>2023.11</strong>: 2 papers are accepted by AAAI 2024 main / AAAI 2024 demo!</li>)

[//]: # (    <li><strong>2023.10</strong>: <font color="red"> I am awarded ByteDance Scholar Fellowship, and Chu Kochen Presidential Scholarship! </font></li>)

[//]: # (    <li><strong>2023.10</strong>: <a href="https://twitter.com/_akhaliq/status/1710112638422642732">UniAudio</a> released!</li>)

[//]: # (    <li><strong>2023.09</strong>: One paper is accepted by EMNLP 2023!</li>)

[//]: # (    <li><strong>2023.07</strong>: One paper is accepted by ACM-MM 2023! </li>)

[//]: # (    <li><strong>2023.06</strong>: One paper is accepted by ICCV 2023! </li>)

[//]: # (    <li><strong>2023.05</strong>: 8 papers are accepted by ACL 2023 &#40;main conference and findings&#41;! Thanks to my co-authors! </li>)

[//]: # (    <li><strong>2023.04</strong>:  <a href="https://github.com/AIGC-Audio/AudioGPT">AudioGPT</a> and <a href="https://github.com/yangdongchao/AcademiCodec">HiFi-Codec</a> released!  </li>)

[//]: # (    <li><strong>2023.04</strong>: One papers is accepted by ICML 2023! </li>)

[//]: # (    <li><strong>2023.02</strong>: Make-An-Audio released! Media coverage: <a href="https://mp.weixin.qq.com/s/fphIJ13RWRIgGNTwYO06bw">Heart of Machine</a>, <a href="https://zhuanlan.zhihu.com/p/605228032">ByteDance</a> and <a href="https://twitter.com/_akhaliq/status/1619589070329348096">Twitter</a> </li>)

[//]: # (    <li><strong>2023.01</strong>: One papers is accepted by ICLR 2023! </li>)

[//]: # (    <li><strong>2022.09</strong>: Two papers are accepted by NeurIPS 2022! </li>)

[//]: # (  </ul>)

[//]: # (</div>)

