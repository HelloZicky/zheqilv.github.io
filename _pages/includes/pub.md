
[//]: # (# üìù Representative Publications )

[//]: # (## Multi-modal Generative AI)

[//]: # (- Spoken Large Language Model: **InstructSpeech &#40;ICML 2024&#41;**, **UniAudio &#40;ICML 2024&#41;**, **AudioGPT &#40;AAAI demo 2024&#41;**, **Make-A-Voice &#40;ACL 2024&#41;**, **HiFi-Codec**)

[//]: # (- Text-to-Audio Synthesis: **Make-An-Audio &#40;ICML 2023&#41;**)

[//]: # (- Text-to-Speech Synthesis: **GenerSpeech &#40;NeurIPS 2022&#41;** for zero-shot text-to-speech, **FastDiff &#40;IJCAI 2022&#41; / ProDiff &#40;ACM-MM 2022a&#41;** for diffusion text-to-speech)

[//]: # (- Singing Voice Synthesis: **SingGAN &#40;ACM-MM 2022b&#41; / Multi-Singer &#40;ACM-MM 2021&#41;**)

[//]: # ()
[//]: # ()
[//]: # (## Multi-modal Language Processing)

[//]: # (- Audio-Visual Speech-to-Speech Translation: **TranSpeech &#40;ICLR 2023&#41; / AV-TranSpeech &#40;ACL 2023&#41;**)

[//]: # (- Self-Supervised Learning: **Prosody-MAE &#40;ACL 2023&#41;**)

[//]: # ()
[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2023</div><img src='images/make-an-audio-arch.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (- [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.]&#40;https://arxiv.org/abs/2304.12995&#41; **Rongjie Huang**, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe. **Arxiv, 2023**)

[//]: # ()
[//]: # (- **Academic / Industry Impact**: Our work are promoted by different media and forums, such as [Heart of Machine]&#40;https://mp.weixin.qq.com/s/pesuhzQ3cfaz-bhxMew46g&#41;, [New Intelligence]&#40;https://mp.weixin.qq.com/s/BXLxD0bboWS96iEHGZ9xTQ&#41;, and [Twitter]&#40;https://twitter.com/_akhaliq/status/1619589070329348096&#41;. We have code released at [![]&#40;https://img.shields.io/github/stars/AIGC-Audio/AudioGPT?style=social&label=Code+Stars&#41;]&#40;https://github.com/AIGC-Audio/AudioGPT&#41; [![Hugging Face]&#40;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue&#41;]&#40;https://huggingface.co/spaces/AIGC-Audio/AudioGPT&#41;.)

[//]: # ()
[//]: # (</div>)

[//]: # (</div>)

[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2023</div><img src='images/make-an-audio-arch.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (- [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models.]&#40;&#41; **Rongjie Huang**, Jiawei Huang, Dongchao Yang, Yi Ren, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao. **ICML, 2023. Hawaii, USA**)

[//]: # ()
[//]: # (- **Academic / Industry Impact**: Our work are promoted by different media and forums, such as [Heart of Machine]&#40;https://mp.weixin.qq.com/s/fphIJ13RWRIgGNTwYO06bw&#41;, [ByteDance]&#40;https://zhuanlan.zhihu.com/p/605228032&#41;, and [Twitter]&#40;https://twitter.com/_akhaliq/status/1619589070329348096&#41;. Code is coming!)

[//]: # ()
[//]: # (</div>)

[//]: # (</div>)

[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/transpeech.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (- [TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation.]&#40;https://arxiv.org/abs/2205.12523&#41; **Rongjie Huang**, Jinglin Liu, Huadai Liu, Yi Ren, Lichao Zhang, Jinzheng He, and Zhou Zhao. **ICLR, 2023. Kigali, Rwanda** )

[//]: # ()
[//]: # (One of our **continuous efforts to reduce communication barrier**, and we have follow-up works: **Audio-Visual S2T [&#40;MixSpeech, ICCV 2023&#41;]&#40;https://arxiv.org/abs/2303.05309&#41;, Audio-Visual S2ST [&#40;AV-TranSpeech, ACL 2023&#41;]&#40;https://arxiv.org/abs/2305.15403&#41;, Multi-modal S2ST, Style-aware S2ST,  Zero-shot S2ST**. Code released: [![]&#40;https://img.shields.io/github/stars/Rongjiehuang/TranSpeech?style=social&label=Code+Stars&#41;]&#40;https://github.com/Rongjiehuang/TranSpeech&#41;. )

[//]: # ()
[//]: # (</div>)

[//]: # (</div>)

[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/generspeech.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (- [GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech.]&#40;https://arxiv.org/abs/2205.07211&#41; **Rongjie Huang**, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. **NeurIPS, 2022. New Orleans, USA**)

[//]: # ()
[//]: # (The first **zero-shot** TTS generalizable to unseen speaker, emotion, and prosody! Media coverage: [PaperWeekly]&#40;https://mp.weixin.qq.com/s/Mp181vfq24m1HqgJqbMnlg&#41;, [Speech Home]&#40;https://mp.weixin.qq.com/s/EXdfb0DUTbB6OHbjDS2u7g&#41;. Code released: [![]&#40;https://img.shields.io/github/stars/Rongjiehuang/GenerSpeech?style=social&label=Code+Stars&#41;]&#40;https://github.com/Rongjiehuang/GenerSpeech&#41;. )

[//]: # ()
[//]: # ()
[//]: # (</div>)

[//]: # (</div>)

[//]: # ()
[//]: # (<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICJAI 2022</div><img src='images/fastdiff.png' alt="sym" width="100%"></div></div>)

[//]: # (<div class='paper-box-text' markdown="1">)

[//]: # ()
[//]: # (- [FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis.]&#40;https://arxiv.org/abs/2204.09934&#41; **Rongjie Huang**, Max W.Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, and Zhou Zhao. **IJCAI, 2022&#40;oral&#41;. Vienna, Austria** )

[//]: # ()
[//]: # (One of our **continuous efforts in generative modeling**, and we have follow-up works: **FastDiff 2, ProDiff**. We release a **diffusion text-to-speech pipeline** [![Hugging Face]&#40;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue&#41;]&#40;https://huggingface.co/spaces/Rongjiehuang/ProDiff&#41; using **ProDiff** [![]&#40;https://img.shields.io/github/stars/Rongjiehuang/prodiff?style=social&label=Code+Stars&#41;]&#40;https://github.com/Rongjiehuang/prodiff&#41; and **FastDiff** [![]&#40;https://img.shields.io/github/stars/Rongjiehuang/FastDiff?style=social&label=Code+Stars&#41;]&#40;https://github.com/Rongjiehuang/FastDiff&#41;. Our work are promoted by different media and forums, such as [Tencent AI Lab]&#40;https://mp.weixin.qq.com/s/GmLzLw3GnDsK0OuUpgEySQ&#41;, [Speech Home]&#40;https://mp.weixin.qq.com/s/BWf_uZdG0icWk5odChxhuA&#41;, and [Twitter]&#40;https://twitter.com/_akhaliq/status/1517308526691065856&#41;, which is a [Trending Project at both Github and Paperwithcode.]&#40;https://twitter.com/pythontrending/status/1528332486257819651&#41; )

[//]: # ()
[//]: # (</div>)

[//]: # (</div>)

# Full Publication List

[//]: # (* denotes co-first authors, # denotes co-supervised)
* denotes co-first authors, &dagger denotes corresponding author

Here‚Äôs the corrected version with **Zheqi Lv** bolded and papers categorized by their respective years:

# 2025

- [Disentangled Knowledge Tracing for Alleviating Cognitive Bias]() Yiyun Zhou*, **Zheqi Lv***, Shengyu Zhang, Jingyuan Chen. **WWW, 2025.**
- [Optimize Incompatible Parameters through Knowledge Integration]() **Zheqi Lv**, KeMing Ye, Shengyu Zhang, Wenqiao Zhang, Wenjie Wang, Tat-Seng Chua, Fei Wu. **AAAI, 2025.**
- [Preliminary evaluation of the Test-Time Training layers in recommendation system]() Tianyu Zhan, **Zheqi Lv**, Shengyu Zhang, Jiwei Li. **AAAI Student Abstract, 2025.**
- [Collaborative Large Language Models and Sequential Recommendation Models for Device-Cloud Recommendation]() **Zheqi Lv**, Tianyu Zhan, Wenjie Wang, Xinyu Lin, Shengyu Zhang, Wenqiao Zhang, Jiwei Li, Kun Kuang, Fei Wu. **KDD, 2025.**
- [Forward Once for All: Structural Parameterized Adaptation for Efficient Cloud-coordinated On-device Recommendation]() Kairui Fu, **Zheqi Lv**, Shengyu Zhang, Fan Wu, Kun Kuang. **KDD, 2025.**


# 2024

- [Backpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration]() Wei Ji*, Li Li*, **Zheqi Lv**&dagger, Wenqiao Zhang, Mengze Li, Zhen Wan, Wenqiang Lei, Roger Zimmermann. **TOMM, 2024.**
- [FedMcon: An Adaptive Aggregation Method for Federated Learning via Meta Controller]() Tao Shen, **Zheqi Lv**, Kun Kuang, Chao Wu, Fei Wu. **Frontiers of Information Technology & Electronic Engineering, 2024.**
- [An Adaptive Aggregation Method for Federated Learning via Meta Controller]() Tao Shen, Zexi Li, Ziyu Zhao, Didi Zhu, **Zheqi Lv**, Shengyu Zhang, Kun Kuang, Fei Wu. **MM Asia Workshop, 2024.**
- [Semantic Codebook Learning for Dynamic Recommendation Models]() **Zheqi Lv**, Shaoxuan He, Tianyu Zhan, Shengyu Zhang, Wenqiao Zhang, Jingyuan Chen, Zhou Zhao, Fei Wu. **MM, 2024.**
- [De-fine: Decomposing and Refining Visual Programs with Auto-Feedback]() Minghe Gao, Juncheng Li, Hao Fei, Liang Pang, Wei Ji, Guoming Wang, **Zheqi Lv**, Wenqiao Zhang, Siliang Tang, Yueting Zhuang. **MM, 2024.**
- [DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation]() Kairui Fu, Shengyu Zhang, **Zheqi Lv**, Jingyuan Chen, Jiwei Li. **KDD, 2024.**
- [Revisiting the Domain Shift and Sample Uncertainty in Multi-source Active Domain Transfer]() Wenqiao Zhang*, **Zheqi Lv**&dagger. **CVPR, 2024.**
- [Intelligent Model Update Strategy for Sequential Recommendation]() **Zheqi Lv**, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, Kun Kuang. **WWW, 2024.**
- [AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation]() Zihao Tang, **Zheqi Lv**, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu, Kun Kuang. **ICLR, 2024.**
- [Learning to Reweight for Generalizable Graph Neural Network]() Zhengyu Chen, Teng Xiao, Kun Kuang, **Zheqi Lv**, Min Zhang, Jinluan Yang, Chengqiang Lu, Hongxia Yang, Fei Wu. **AAAI, 2024.**

# 2023

- [DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization]() **Zheqi Lv**, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, Fei Wu. **WWW, 2023.**
- [Parameters Efficient Fine-Tuning for Long-Tailed Sequential Recommendation]() **Zheqi Lv***, Feng Wang*, Shengyu Zhang, Kun Kuang, Hongxia Yang, Fei Wu. **CICAI, 2023.**
- [ART: rule bAsed futuRe-inference deducTion]() Mengze Li, Tianqi Zhao, Jionghao Bai, Baoyi He, Jiaxu Miao, Wei Ji, **Zheqi Lv**, Zhou Zhao, Shengyu Zhang, Wenqiao Zhang, Fei Wu. **EMNLP, 2023.**
- [Federated Mutual Learning: A Collaborative Machine Learning Method for Heterogeneous Data, Models and Objectives]() Tao Shen, Jie Zhang, Xinkang Jia, Fengda Zhang, **Zheqi Lv**, Kun Kuang, Chao Wu, Fei Wu. **Frontiers of Information Technology & Electronic Engineering, 2023.**

# 2022 and prior

- [Health status prediction for the elderly based on machine learning]() Fang-Yu Qin*, Zhe-Qi Lv*, Dan-Ni Wang, Bo Hu, Chao Wu. **Archives of Gerontology and Geriatrics, 2020.**
- [A Brief Survey of Data Pricing for Machine Learning]() Zuoqi Tang, **Zheqi Lv**, Chao Wu. **CS & IT Conference Proceedings, 2020.**


[//]: # (# 2025)

[//]: # ()
[//]: # (- [OmniSep: Unified Omni-modal Sound Separation.]&#40;&#41; Xize Cheng, Zehan Wang, Ziang Zhang, **Rongjie Huang**, Jialung Zuo, Shengpeng Ji, Ziyang Ma, Siqi Zheng, Tao Jin, Zhou Zhao. **ICLR, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers.]&#40;&#41; Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, **Rongjie Huang**, Shijie Geng, Renrui Zhang, Junlin Xie, Wenqi Shao, Zhengkai Jiang, Tianshuo Yang, Weicai Ye. **ICLR, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation.]&#40;&#41; Zhenhui Ye, Jinzheng He, Ziyue Jiang, **Rongjie Huang**, Jiawei Huang, Jinglin Liu, Yi Ren, Xiang Yin, Zejun MA, Zhou Zhao. **ICLR, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling.]&#40;&#41; Shengpeng Ji, Ziyue Jiang, Wen Wang, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Xize Cheng, Zehan Wang, Ruiqi Li, Ziang Zhang, Xiaoda Yang, **Rongjie Huang**, Yidi Jiang, Qian Chen. **ICLR, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [Improving Multi-modal Representations via Binding Space in Scale.]&#40;&#41; Zehan Wang, Ziang Zhang, Minjie Hong, Hang Zhang, Luping Liu, **Rongjie Huang**, Xize Cheng, Shengpeng Ji, Tao Jin, Hengshuang Zhao, Zhou Zhao.  **ICLR, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?]&#40;&#41; Xize Cheng, Ruofan Hu, Xiaoda Yang, Jingyu Lu, Dongjie Fu, Zehan Wang, Shengpeng Ji, Rongjie Huang, Boyang Zhang, Tao Jin, Zhou Zhao.  **ICLR, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [TechSinger: Technique Controllable Multilingual Singing Voice Synthesis via Flow Matching.]&#40;&#41; Wenxiang Guo, Yu Zhang, Changhao Pan, **Rongjie Huang**, Li Tang, Ruiqi Li, Zhiqing Hong, Yongqi Wang, Zhou Zhao. **AAAI, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [3D-Speaker-Toolkit: An Open-Source Toolkit for Multimodal Speaker Verification and Diarization.]&#40;&#41; Yafeng Chen, Siqi Zheng, Hui Wang, **Rongjie Huang**, Qian Chen, Shiliang Zhang, Wen Wang, Xihao Li. **ICASSP, 2025**)

[//]: # ()
[//]: # ()
[//]: # (- [NAT3DSound: 3D Spatial Sound Field Synthesis with Multi-Modal Non-Autoregressive Transformer.]&#40;&#41; Fuming You, **Rongjie Huang**, Boyang Zhang, Yongqi Wang, Zhiqing Hong, Qian Yang, Zhimeng Zhang, Zhou Zhao. **ICASSP, 2025**)

[//]: # ()
[//]: # ()
[//]: # ()
[//]: # (# 2024)

[//]: # ()
[//]: # ()
[//]: # ()
[//]: # (- [InstructSpeech: Following Speech Editing Instructions via Large Language Models.]&#40;&#41; **Rongjie Huang**, Ruofan Hu, Yongqi Wang, Zehan Wang, Xize Cheng, Ziyue Jiang, Zhenhui Ye, Dongchao Yang, Luping Liu, Peng Gao, Zhou Zhao. **ICML, 2024.**)

[//]: # ()
[//]: # (- [Make-A-Voice: Multilingual Unified Voice Generation With Discrete Representation at Scale.]&#40;&#41; **Rongjie Huang**, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Jinchuan Tian, Luping Liu, Zhenhui Ye, Ziyue Jiang, Xuankai Chang, Jiatong Shi, Chao Weng, Zhou Zhao, Dong Yu. **ACL, 2024.**)

[//]: # ()
[//]: # (- [AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.]&#40;https://arxiv.org/abs/2304.12995&#41; **Rongjie Huang**, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe. **AAAI demo, 2024**)

[//]: # ()
[//]: # (- [UniAudio: An Audio Foundation Model Toward Universal Audio Generation.]&#40;&#41; Dongchao Yang, Jinchuan Tian, Xu Tan, **Rongjie Huang**, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, Zhou Zhao, Shinji Watanabe, Helen M. Meng. **ICML, 2024.**)

[//]: # ()
[//]: # (- [Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion.]&#40;&#41; Zehan Wang, Ziang Zhang, Xize Cheng, **Rongjie Huang**, Luping Liu, Zhenhui Ye, Haifeng Huang, Yang Zhao, Tao Jin, Peng Gao, Zhou Zhao. **ICML, 2024.**)

[//]: # ()
[//]: # (- [InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt.]&#40;&#41; Dongchao Yang, Songxiang Liu, **Rongjie Huang**, Guangzhi Lei, Chao Weng, Helen Meng, Dong Yu. **IEEE Transactions on Acoustics, Speech, and Signal Processing.**)

[//]: # ()
[//]: # (- [Robust Singing Voice Transcription Serves Synthesis.]&#40;&#41; Ruiqi Li, Yu Zhang, Yongqi Wang, Zhiqing Hong, **Rongjie Huang**, Zhou Zhao.  **ACL, 2024.**)

[//]: # ()
[//]: # (- [Text-to-Song: Towards Controllable Music Generation Incorporating Vocal and Accompaniment.]&#40;&#41; Zhiqing Hong, **Rongjie Huang**, Xize Cheng, Yongqi Wang, Ruiqi Li, Fuming You, Zhou Zhao, Zhimeng Zhang.  **ACL, 2024.**)

[//]: # ()
[//]: # (- [TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation.]&#40;&#41; Xize Cheng, **Rongjie Huang**, Linjun Li, Tao Jin, Zehan Wang, Aoxiong Yin, Minglei Li, Xinyu Duan, changpeng yang, Zhou Zhao.  **ACL finding, 2024.**)

[//]: # ()
[//]: # (- [Wav2SQL: Direct Generalizable Speech-To-SQL Parsing.]&#40;&#41; Huadai Liu, **Rongjie Huang**, Jinzheng He, Ran Shen, Gang Sun, Xize Cheng and Zhou Zhao. **ACL finding, 2024.**)

[//]: # ()
[//]: # (- [Self-Supervised Singing Voice Pre-Training towards Speech-to-Singing Conversion.]&#40;&#41; Ruiqi Li, **Rongjie Huang**, Yongqi Wang, Zhiqing Hong, Zhou Zhao. **ACL finding, 2024.**)

[//]: # ()
[//]: # (- [Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis.]&#40;&#41; Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, **Rongjie Huang**, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun MA, Zhou Zhao. **ICLR, 2024.**)

[//]: # ()
[//]: # (- [StyleSinger: Style Transfer for Out-Of-Domain Singing Voice Synthesis.]&#40;&#41; Yu Zhang#, **Rongjie Huang**, Ruiqi Li, Jinzheng He, Yan Xia, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao. **AAAI, 2024.**)

[//]: # ()
[//]: # (- [Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt]&#40;&#41; Yongqi Wang#, Ruofan Hu#, **Rongjie Huang**, Zhiqing Hong, Ruiqi Li, Wenrui Liu, Fuming You, Tao Jin, Zhou Zhao. **NAACL, 2024.**)

[//]: # ()
[//]: # (- [EchoAudio: Efficient and High-Quality Text-to-Audio Generation with Minimal Inference Steps]&#40;&#41; Huadai Liu, **Rongjie Huang**, Yang Liu, Hengyuan Cao, Jialei Wang, Xize Cheng, Siqi Zheng, Zhou Zhao. **ACMMM, 2024.**)

[//]: # ()
[//]: # ()
[//]: # (# 2023)

[//]: # ()
[//]: # (- [Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models.]&#40;&#41; **Rongjie Huang**, Jiawei Huang, Dongchao Yang, Yi Ren, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao. **ICML, 2023. Hawaii, USA**)

[//]: # ()
[//]: # (- [Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias.]&#40;&#41; Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, **Rongjie Huang**, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao. **Arxiv**)

[//]: # ()
[//]: # (- [Make-An-Audio 2: Improving Text-to-Audio with Dual Text Information Representation.]&#40;&#41; Jiawei Huang#, Yi Ren, **Rongjie Huang**, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, Zhou Zhao. **Arxiv, 2023**)

[//]: # ()
[//]: # (- [TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation.]&#40;https://arxiv.org/abs/2205.12523&#41; **Rongjie Huang**, Jinglin Liu, Huadai Liu, Yi Ren, Lichao Zhang, Jinzheng He, and Zhou Zhao. **ICLR, 2023. Kigali, Rwanda** )

[//]: # ()
[//]: # (- [AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation.]&#40;&#41; **Rongjie Huang**, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin and Zhou Zhao. **ACL, 2023** )

[//]: # ()
[//]: # (- [MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition.]&#40;&#41; Xize Cheng*, Linjun Li*, Tao Jin*, **Rongjie Huang***, Wang Lin, Zehan Wang, Huangdai Liu, Ye Wang, Aoxiong Yin, Zhou Zhao. **ICCV, 2023** )

[//]: # ()
[//]: # (- [CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training.]&#40;&#41; Zhenhui Ye*, **Rongjie Huang**, Yi Ren, Ziyue Jiang, Jinglin Liu, Jinzheng He, Xiang Yin and Zhou Zhao. **ACL, 2023** )

[//]: # ()
[//]: # (- [UniSinger: Unified End-to-End Singing Voice Synthesis With Cross-Modality Information Matching.]&#40;&#41; Zhiqing Hong#, Chenye Cui, **Rongjie Huang**, Lichao Zhang, Jinglin Liu, Jinzheng He, Zhou Zhao. **ACM MM, 2023**)

[//]: # ()
[//]: # (- [AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment.]&#40;&#41; Ruiqi Li#, **Rongjie Huang**, Lichao Zhang, Jinglin Liu, Zhou Zhao. **ACL finding, 2023**)

[//]: # ()
[//]: # (- [RMSSinger: Realistic-Music-Score based Singing Voice Synthesis.]&#40;&#41; Jinzheng He, Jinglin Liu, Zhenhui Ye, **Rongjie Huang**, Chenye Cui, Huadai Liu, Zhou Zhao. **ACL finding, 2023**)

[//]: # ()
[//]: # (- [FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models.]&#40;&#41; Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, **Rongjie Huang**, Yi Ren, Zhou Zhao. **ACL finding, 2023**)

[//]: # ()
[//]: # (- [Contrastive Token-Wise Meta-Learning for Unseen Performer Visual Temporal-Aligned Translation.]&#40;&#41; Linjun Li, Tao Jin, Xize Cheng, Ye Wang, Wang Lin, **Rongjie Huang**, Zhou Zhao. **ACL finding, 2023**)

[//]: # ()
[//]: # (- [ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer.]&#40;&#41; Huadai Liu, **Rongjie Huang**, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, Zhou Zhao. **EMNLP, 2023**)

[//]: # ()
[//]: # ()
[//]: # (# 2022)

[//]: # ()
[//]: # (- [GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech.]&#40;https://arxiv.org/abs/2205.07211&#41; **Rongjie Huang**, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. **NeurIPS, 2022. New Orleans, USA**)

[//]: # ()
[//]: # (- [Prosody-TTS: Self-Supervised Prosody Pretraining with Latent Diffusion For Text-to-Speech.]&#40;&#41; **Rongjie Huang**, Chunlei Zhang, Yi Ren, Zhou Zhao, Dong Yu. **ACL finding, 2023**)

[//]: # ()
[//]: # (- [FastDiff 2: Dually Incorporating GANs into Diffusion Models for High-Quality Speech Synthesis.]&#40;&#41; **Rongjie Huang**, Yi Ren, Jinglin Liu, Luping Liu, Zhou Zhao. **ACL finding, 2023**)

[//]: # ()
[//]: # (- [FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis.]&#40;https://arxiv.org/abs/2204.09934&#41; **Rongjie Huang**, Max W.Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, and Zhou Zhao. **IJCAI, 2022&#40;oral&#41;. Vienna, Austria** )

[//]: # ()
[//]: # (- [ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech.]&#40;&#41; **Rongjie Huang**, Zhou Zhao, Huadai Liu, Jinglin Liu, and Yi Ren. **ACM MM, 2022. Lisbon, Portugal**)

[//]: # ()
[//]: # (- [M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus.]&#40;https://arxiv.org/abs/2205.07211&#41; Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, **Rongjie Huang**, Jieming Zhu, Xiao Chen, and Zhou Zhao. **NeurIPS, 2022. New Orleans, USA**)

[//]: # ()
[//]: # (- [VarietySound: Timbre-Controllable Video to Sound Generation via Unsupervised Information Disentanglement.]&#40;&#41; Chenye Cui, Yi Ren, Jinglin Liu, **Rongjie Huang**, Zhou Zhao. **ICASSP, 2023**)

[//]: # ()
[//]: # (# 2021)

[//]: # ()
[//]: # (- [Multi-Singer: Fast multi-singer singing voice vocoder with a large-scale corpus.]&#40;https://dl.acm.org/doi/abs/10.1145/3474085.3475437&#41; **Rongjie Huang**, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. **ACM MM, 2021&#40;oral&#41;. Chengdu, China** )

[//]: # (| [**Project**]&#40;https://multi-singer.github.io/&#41; | [![]&#40;https://img.shields.io/github/stars/Rongjiehuang/multi-singer?style=social&label=Code+Stars&#41;]&#40;https://github.com/Rongjiehuang/multi-singer&#41;)

[//]: # ()
[//]: # (- [EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model.]&#40;https://arxiv.org/abs/2106.09317&#41; Chenye Cui, Yi Ren, Jinglin Liu, Feiyang Chen, **Rongjie Huang**, Mei Li, and Zhou Zhao. **Interspeech, 2021**)

[//]: # ()
[//]: # (- [Bilateral Denoising Diffusion Models.]&#40;https://arxiv.org/abs/2108.11514&#41; Max W.Y. Lam, Jun Wang, **Rongjie Huang**, Dan Su, Dong Yu. **Preprint**)

[//]: # ()
[//]: # ()
[//]: # (# 2020 and Prior)

[//]: # ()
[//]: # (- [SingGAN: Generative Adversarial NetWork For High-Fidelity Singing Voice Generation.]&#40;https://arxiv.org/abs/2110.07468&#41; **Rongjie Huang**, Chenye Cui, Feiyang Chen, Yi Ren, Jinglin Liu, and Zhou Zhao. **ACM MM, 2022. Lisbon, Portugal** )
[//]: # (| [**Project**]&#40;https://singgan.github.io/&#41; )